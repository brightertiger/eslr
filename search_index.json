[["index.html", "ESLR Notes 1 Notes", " ESLR Notes brightertiger 1 Notes Hi I am brightertiger! I am a data scientist looking to expand my knowledge about ML. This is a collection of notes based mostly the book Elements of Statistical Learning. "],["linear-methods-for-regression.html", "2 Linear Methods for Regression", " 2 Linear Methods for Regression 2.0.1 Derivation \\(y = X\\beta + \\epsilon\\) \\(\\epsilon \\sim N(0, \\sigma^2)\\) Linear Function Approximation \\(E(Y|X) = f(X) = \\beta_0 + \\sum \\beta_j x_j\\) Model is linear in parameters Minimize Residual Sum of Squares \\(RSS = \\sum (y_i - f(x_i))^2 = (y - X\\beta)^T(y - X\\beta)\\) Optimal value of beta: \\({\\delta RSS \\over \\delta \\beta} = 0\\) \\(\\hat \\beta = (X^T X)^{-1}(X^Ty)\\) \\(\\hat y = X \\hat \\beta = (X(X^T X)^{-1}X^T)y = H y\\) H is the projection or Hat matrix 2.0.2 Sampling Distribution of \\(\\beta\\) Deviations around the conditional mean are Gaussian \\(Var(\\hat \\beta) = (X^T X)^{-1} \\sigma^2\\) Estimate of Sigma can be done by looking at sample variance \\(\\hat \\sigma^2 = {1 \\over N-p} \\sum (y_i - \\hat y_i)^2\\) \\(\\hat \\beta \\sim N(\\beta, (X^T X)^{-1} \\sigma^2)\\) 2.0.3 Statistical Significance \\(Z_i = {\\beta_i \\over SE_i} = \\frac{\\beta_i}{\\hat \\sigma \\sqrt v_i}\\) v is the diagnoal element of \\((X^T X)^{-1}\\) Testing significance for a group of parameters Say categorical variables with all k variables \\(F = (RSS_0 - RSS_1) / (RSS_1 / N - p)\\) Change is RSS of the bigger model normalized by the estimate of variance 2.0.4 Gauss-Markov Theorem Among all the unbiased estimators, the least square estimates have lowest variance \\(E(Y_0 - \\hat Y_0))^2 = \\sigma^2 + MSE(\\hat f(X_0))\\) 2.0.5 Subset Selection Select only a few variables for better interpretability Best subset selection os size K is the one that yields minimum RSS Forward Selection Sequentially add one variable that most improves the fit QR decomposition / successive orthogonalization to look at correlation Backward Selection Sequentially delete the variable that has least impact on the fit Z Score Hybrid Stepwise Selection Consider both forward and backward moves at each step AIC for weighting the choices Forward Stagewise Selection Add the variable most correlated with current residual Don’t re-adjust the coefficients of the existing variables 2.0.6 Shrinkage Methods Shinkage methods result in biased estimators but a large reduction in variance More continuous and dont suffer from high variability Ridge Regression Impose a penalty the size of the coefficicents \\(\\hat \\beta^{\\text{ridge}} = \\arg \\min (y - X \\beta)^T(y - X \\beta) + \\lambda \\sum \\beta^2\\) \\(\\hat \\beta^{\\text{ridge}} = \\arg \\min (y - X \\beta)^T(y - X \\beta) \\; \\text{subject to} \\sum \\beta^2 \\le t\\) t is the budget In case of correlated variables, coefficcients are poorly determined A large positive coefficient of a variable is canceled by a large negative coefficient of the correlated variable Solution not invariant to scaling. Standardize the inputs and don’t impose penalty on intercept \\(\\hat \\beta^{\\text{ridge}} = (X^T X + \\lambda I)^{-1}(X^Ty)\\) In case of correlated predictors, the original \\((X^T X)\\) wasn’t full rank. But by adding noise to diagonal elements, the matrix can now be inverted. In case of orthonormal inputs (PCA), the ridge coefficicents are scaled versions of the original least-square estimates. \\(\\lambda\\) controls the degrees of freedom. A large value results in effectively dropping the variables. Lasso Regression \\(\\hat \\beta^{\\text{ridge}} = \\arg \\min (y - X \\beta)^T(y - X \\beta) + \\lambda \\sum |\\beta|\\) Non-linear optimization A heavy restriction on budget makes some coefficients exactly zero Continuous subset selection Comparison between Ridge and Laso Ridge represents a disk \\(\\beta_1^2 + \\beta_2^2 &lt;= t\\) Lasso represents a rhombus \\(|\\beta_1| + |\\beta_2| &lt;= t\\) At optimal value, the estimated parameters can be exactly zero (corner solutions) Bayesian MAP estimates with different priors Lasso has Laplace Prior Ridge has Gaussian Prior Elastic Net \\(\\lambda \\sum \\alpha \\beta^2 + (1 - \\alpha) |\\beta|\\) Variable selection like Lasso Shinking coefficients like Ridge 2.0.7 Partial Least Squares Alternative approach to PCA deal with correlated features Supervised transformation Principal component regression seeks directions that have high variance Partial Least Square seeks direction with high variance and high correlation with response Derive new features by linear combination of raw variables re-weighted by the correlation "],["linear-methods-for-classification.html", "3 Linear Methods for Classification", " 3 Linear Methods for Classification 3.0.1 Decision Boundary Classificaiton approach is to learn a discriminant function \\(\\delta_k(x)\\) for each class Classify x to the class with largest discriminant value The decision boundary is linear if \\(\\delta_k(x)\\) is linear Posterior prbability is linear \\(P(G=k|X=x)\\) Their monotonic transformation is linear Linear Decision Boundary: \\(f_k(x) = \\beta_{0k} + \\beta_k x\\) Decision Boundary between two classes (k, l) is the set of points where \\(f_k(x) = f_l(x)\\) \\(\\{x : (\\beta_{0k} - \\beta_{0l}) + (\\beta_k - \\beta_l) x = 0\\}\\) Affine set or a hyperplane Example: Binary Logistic Regression \\(P({G=1 \\over X=x}) = \\frac{\\exp(\\beta x)}{1 + \\exp(\\beta x)}\\) \\(P({G=0\\over X=x}) = \\frac{1}{1 + \\exp(\\beta x)}\\) \\(\\log({P(G=1 | X=x) \\over p(G=0 | X=x)}) = x \\beta\\) Log-odds transformation gives linear decision boundary Decsion boundary is the set of points \\(\\{x| \\beta x = 0\\}\\) 3.0.2 Linear Probability Model Encode each of the k classes with an indicator function \\(Y_{N \\times K}\\) Fit a regression model to each of the classes simulatneously \\(\\hat \\beta = (X&#39;X)^{-1}(X&#39;Y)\\) \\(\\hat Y = X \\hat \\beta\\) Drawbacks Predictions can be outside range (0,1) Classes can be masked by others Large number of classes with small number of features Possible that one of the classes (say 2) gets dominated thoughout by the other classes (1,3) The model will never predict for class 2 3.0.3 Linear and Quadratic Discriminant Analysis Bayes theorem posterior \\(\\propto\\) prior x likelihood \\(P(G = k | X= x) = \\frac{f_k(x) \\times \\pi_k}{\\sum_k f_k(x) \\times \\pi_k}\\) \\(f_k(x)\\) is the discriminant function \\(\\pi_k\\) is the prior estimate Naive Bayes assumes each of the class densities are product of marginal densities. Inputs are conditionally independent of each class LDA (and QDA) assumes the discriminant function to have MVN probability density function LDA makes the assumption that the covariance martix (for MVN) is common for all the classes Discrimination Function \\(f_k(x) = \\frac{1}{(2\\pi)^{p/2} \\Sigma^{1/2}} \\exp\\{(X - \\mu)^T \\Sigma^{-1} (X - \\mu)\\}\\) Decision Boundary \\(\\log(\\frac{P(G=k | X=x)}{P(G=l | X=x)}) = C + X^T \\Sigma^{-1}(\\mu_k - \\mu_l)\\) Linear in X The constant terms can be grouped together because of common covariance matrix Estimation \\(\\pi_k = N_k / N\\) \\(\\mu_k = \\sum_{i \\in K} x_i / N_k\\) \\(\\Sigma = \\sum_k \\sum_{i \\in K} (x_i - \\mu_k)^T(x_i - \\mu_k) / N_k\\) QDA relaxes the assumtion of contant covariance matrix It assumes a class specific covariance matrix Discrimination function becomes quadratic in x The number of parameters to be estimated grows considerably Regularization Compromise between LDA and QDA Shrink the individual covariances of QDA towards LDA \\(\\alpha \\Sigma_k + (1 - \\alpha) \\Sigma\\) Computation Simplify the calculation by using eigen decomposition of the covariance matrix \\(\\Sigma\\) 3.0.4 Logistic Regression Model posterior probabilities via separate functions while ensuring the output remains in the range [0,1] \\(P({G=1 \\over X=x}) = \\frac{\\exp(\\beta x)}{1 + \\exp(\\beta x)}\\) \\(P({G=0\\over X=x}) = \\frac{1}{1 + \\exp(\\beta x)}\\) Estimation is done by maximizing conditional log-likelihood \\(LL(\\beta) = \\sum y_i(\\log(p(x_i, \\beta)) + (1 - y_i) (1 - \\log(p(x_i, \\beta))\\) \\(LL(\\beta) = \\sum y (x \\beta) + \\log(1 + \\exp x \\beta)\\) Normal Equation \\(\\frac{\\delta LL}{\\delta \\beta} = \\sum x_i (y_i - p(x_i, \\beta)) = 0\\) Optimization Non-linear function of parameters Use Newton-Raphson method Seond Order Derivative or Hessian \\(\\frac{\\delta^2 LL}{\\delta \\beta^2} = \\sum x_i x_i^T p(x_i, \\beta) (1 - p(x_i, \\beta))\\) The second order derivative is positive, hence it’s a convex optimization problem IRLS (Iteratively Weighted Least Squares) algorithm Goodness of Fit Deviance = \\(-2 (\\log L_M - \\log L_S)\\) L(M): LL of Current Model L(S) LL of Saturated Model Model that perfectly fits the data, Constant for a given dataset Compare two different models by looking at change in deivance Regularization Lasso penalties can be added to the objective function Intercept term isn’t penalized 3.0.5 Comparison between LDA and Logistic Regression Both Logistic and LDA return linear decision boundaries Difference lies in the way coefficients are estimated Logistic Regression makes less stringent assumptions LR maximizes conditional log-likelihood LDA maximizes full log-likelihood (i.e. joint desnity) LDA makes more restrictive assumptions about the distributions Efficiency is estimation Less robust to outliers 3.0.6 Percepton Learning Algorithm Minimize the distance of missclassified points to the separating hyperplane \\(D(\\beta) = - \\sum y (x^T \\beta)\\) Use SGD to estimate the parameters When the data is separable, there are many solutions that exist. The final convergence depends on the initialization. When data isn’t separable, there is no convergence. 3.0.7 Maximum Margin Classifiers Maximize the distance of of points from either class to the hyperplane. \\(L = \\max_{\\beta, ||\\beta|| = 1} M \\, \\, \\text{subject to} \\, y_i \\times x_i \\beta &gt;= M \\, \\forall \\, i \\in N\\) The final parameters can be arbitrarily scaled. \\(L = \\max {1 \\over 2}||\\beta||^2 \\, \\, \\text{subject to} \\, y_i \\times x_i \\beta &gt;= 1 \\, \\forall \\, i \\in N\\) Lagrangian Multiplier \\(L = \\max {1 \\over 2}||\\beta||^2 - \\sum \\alpha_i (y_i \\times x_i \\beta) - 1)\\) Taking derivative wrt to \\(\\beta\\) \\(\\beta = \\sum \\alpha_i y_i x_i\\) Parameter is a linear combination of points where the constraints are active \\(\\alpha_i &gt; 0\\) "],["kernel-methods.html", "4 Kernel Methods", " 4 Kernel Methods 4.0.1 Kernel Density Estimation A random sample \\([x_0, x_1, ... x_n]\\) is drawn from probability distribution \\(f_X(x)\\) Parzen Estimate of \\(\\hat f_X(x)\\) \\(\\hat f_X(x_0) = {1 \\over N \\lambda} \\,\\, \\# x_i \\in N(x_0)\\) \\(\\lambda\\) is width of the neighbourhood The esitmate is bumpy Gaussian Kernel of width \\(\\lambda\\) can be a choice of Kernel \\(\\hat f_X(x_0) = {1 \\over N \\lambda} K_{\\lambda}(x_i, x_0)\\) \\(K_{\\lambda}(x_i, x_0) = \\phi(\\|x_i - x_0\\|) / \\lambda\\) Weight of point \\(x_i\\) descreases as distance from \\(x_0\\) increases New estimate for density is \\(\\hat f_X(x_0) = {1 \\over N } \\phi_{\\lambda}(x_ - x_0)\\) Convolution of Sample empirical distribution with Gaussian Kernel 4.0.2 Kernel Desnity Classification Bayes’ Theorem \\(P(G=j | X=x_0) \\propto \\hat \\pi_j \\hat f_j(x_0)\\) \\(\\hat \\pi_j\\) is the sample proportion of the class j \\(\\hat f_j(x_0)\\) is the Kernel density estimate for class j Learning separate class densities may be misleading Dense vs Non-dense regions in feature space Density estimates are critical only near the decision boundary 4.0.3 Naive Bayes Classifier Applicable when dimension of feature space is high Assumption: For a given class, the features are independent \\(f_j(x) = \\prod_p f_{jp}(x_p)\\) Rarely holds true in real world dataset \\(\\log \\frac{P(G=i|X=X)}{P(G=j|X=X)} = \\log \\frac{\\pi_i \\prod f_{ip}(x_p)}{\\pi_j \\prod f_{jp}(x_p)}\\) \\(\\log \\frac{\\pi_i}{\\pi_j} + \\sum \\log \\frac{f_{ip}(x_p)}{f_{jp}(x_p)}\\) 4.0.4 Radial Basis Functions Basis Functions \\(f(x) = \\sum \\beta h(x)\\) Transform lower dimension features to high dimensions Data which is not linearly separable in lower dimension may become linearly separable in higher dimensions RBF treats gaussian kernel functions as basis functions Taylor series expansion of \\(\\exp(x)\\) Polynomial basis function with infinite dimensions \\(f(x) = \\sum_j K_{\\lambda_j}(\\xi_j, x) \\beta_j\\) \\(f(x) = \\sum_j D({\\|x_i - \\xi_j\\| \\over \\lambda _j}) \\beta_j\\) D is the standard normal gaussian density function For least square regression, SSE can be optimized wrt to \\(\\beta, \\xi, \\lambda\\) Non-Linear Optimization Use greedy approaches like SGD Simplify the calculations by assuming \\(\\xi, \\lambda\\) to be hyperparameters Use unsupervised learning to estimate them Assuming constant variance simplifies calculations It can create “holes” where none of the kernels have high density estimate 4.0.5 Mixture Models Extension of RBF \\(f(x) = \\sum_j \\alpha_j \\phi(x, \\mu_j, \\Sigma_j)\\) \\(\\sum \\alpha_j = 1\\), are the mixing proporitons Gaussian mixture models use Gaussian kernel in place of \\(\\phi\\) Parameters are fit using Maximum Likelihood If the covariance martix is restricted to a diagonal matrix \\(\\Sigma = \\sigma^2 I\\), then it reduces to radial basis expansion Classification can be done via Bayes Theorem Separate density estimation for each class Probability is \\(\\propto \\hat \\pi_i f_j(x)\\) "],["model-selection.html", "5 Model Selection", " 5 Model Selection 5.0.1 Bias-Variance Tradeoff Generalization Prediction error over an independent test sample \\(\\text{ERR}_T = E(L(Y, \\hat f(x)) | T)\\) T refers to the training set used to build the model L is the loss function used to evaluate the model performance Regression: Squared Loss, Absolute Loss Classification: 0-1 Loss, Deviance (-2 x LL) As the model becomes more complex, it adaps to complex underlying structure of the training data Decrease in bias but increase in variance If the underlying training data changes, the complex fitted model will change to a large extent Intermediate model complexity that gives minimum expected test error Training error is not a good estimate of test error Consistently decreases with increasing model complexity Poor generalization Model Selection Estimating Performance of different models to choose the best one Model Assessment Having selected the model, estimating the generalization error on new, unseen data Divide the dataset Training: Fit the models Validaiton: Estimate model prediction error for model selection Test: Generalizaiton error of the final chosen model 5.0.2 Bias-Variance Decomposition \\(Y = f(X) + \\epsilon\\) \\(E(\\epsilon) = 0, V(\\epsilon) = \\sigma^2_{\\epsilon}\\) \\(\\text{ERR}(x_0) = E((Y - \\hat f(x_0))^2 | x_0)\\) \\(\\text{ERR}(x_0) = E((f(x_0) + \\epsilon - \\hat f(x_0))^2)\\) \\(\\text{ERR}(x_0) = \\sigma^2_{\\epsilon} + [E(\\hat f(x_0) - f(x_0))]^2 + E[\\hat f(x_0) - E(\\hat f(x_0)]^2\\) MSE = Irreducible Error + Bias Squared + Variance Bias: Difference between average of estimate and true mean Variance: Squared Deviation of model around its mean More Complex Model Lower Bias Higher Variance For linear Model \\(\\text{Variance} \\propto p\\) Complexity of the model is related to the number of parameters \\(\\text{Bias}^2 = \\text{Model Bias}^2 + \\text{Estimation Bias}^2\\) Model Bias: Best fitting linar model and True function Estimation Bias: Estimated Model and Best fitting linar model For OLS: Estimation Bias is 0, BLUE For Ridge: Estimation Bias is positive Trade-off with reduction in variance 5.0.3 Optimism of Training Error \\(\\text{ERR} = E(\\text{ERR}_T)\\) Training error is less than test error Same data is being used to train and evaluate the model Optimistic estimate of generalization error \\(\\text{ERR}_{in}\\): Error between sample and populaiton regression function estimates on training data \\(\\bar{\\text{err}}\\) Average sample regression error over training data Optimisim in training error estimate \\(\\text{op} = \\text{ERR}_{in} - \\bar{\\text{err}}\\) Related to \\(\\text{cov}(y, \\hat y)\\) How strongly a label value affects its own prediction Optimism increases with number of inputs Optimism decreases with number of training samples 5.0.4 In-sample Prediciton Error \\(\\text{ERR}_{in} = \\bar{\\text{err}} + \\text{op}\\) Cp Statistic \\(C_p = \\bar{\\text{err}} + 2{p \\over N} \\sigma^2_{\\epsilon}\\) p is the effective number of parameters AIC \\(\\text{AIC} = {-2 \\over N} LL + 2{p \\over N}\\) p is the effective number of parameters For model selection, choose the one with lowest AIC Effective Number of Parameters Linear Regression: \\(\\hat y = ((X&#39;X)^{-1}X&#39;)y\\) Ridge Regression: \\(\\hat y = (((X&#39;X)^{-1} + \\lambda I)X&#39;)y\\) Generalized Form: \\(\\hat y = S y\\) p is the trace of the S Matrix BIC \\(\\text{BIC} = {-2 \\over N} LL + \\log N \\times p\\) Penalizes complex models more heavily compared to AIC Bayesian Approach \\(P(M |D) \\propto P(D |M) P(M)\\) Laplace Approximation \\(\\log P(D |M) = \\log P(D |M, \\theta) - p \\log N\\) \\(\\log P(D |M, \\theta)\\) is the MLE objective function Compare two models \\(P(M1 |D) / P(M2 |D) = P(M1) / P(M2) + P(D | M1) / P(D | M2)\\) The first term is constant (non-informative priors) The second term the Bayes Factor 5.0.5 VC Dimension AIC, C-p statistic need the information on model complexity Effective number of parameters Difficult to estimate for non-linear models VC Dimension is Generalized Model Complexity of a class of functions How “wiggly” can the memeber of this class be? Shattering Points that can be perfectly separated by a class of functions, no matter how the binary labels are assigned VC Dimension: Largest number of points that can be shattered by members of class of functions 3 points in case of linear classifier in a plane 4 points can lead to XOR 5.0.6 Cross Validation Estimation for \\(\\text{ERR}_T\\) Data scarce situation Divide data into K equal parts Indexing function: \\(\\kappa : \\{1,2,....N\\} \\rightarrow \\{1, 2 ... K\\}\\) Fit model on K-1 parts and predict on Kth part Cross Validaiton Error \\(CV(f) = {1 \\over N}\\sum L(y_i, \\hat y_i^{f_{-\\kappa}})\\) 5-fold, 10-fold cross validation is recommended 5.0.7 Boostrap Methods Estimation for \\(\\text{ERR}\\) Randomly draw datasets from training data by sampling with replacement Each dataset has the same size of original training data Fit the model on each of the bootstrap datasets \\(\\text{ERR}_{\\text{boot}} = {1 \\over B}{1 \\over N} \\sum_B \\sum_N L(y, \\hat y)\\) Bootstrap uses overlapping samples across model fits (unlike cross validation) \\(P(i \\in B) = 1 - (1 - {1 \\over N})^N \\approx 1 - e^{-1}\\) \\(\\text{ERR}_{\\text{boot}}\\) isn’t a good estimator becuase of leakage Use Out-of-bag error instead Samples which have been dropped by boostrap "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
